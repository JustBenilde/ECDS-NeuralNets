{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define the path where you want to download the dataset\n",
    "# desired_path = \"C:/repos/ECDS-NeuralNets/Day 1/Dataset\"\n",
    "\n",
    "# # Download laval version\n",
    "# default_path = kagglehub.dataset_download(\"uciml/iris\")\n",
    "\n",
    "# # Move the downloaded files to your desired directory\n",
    "# shutil.move(default_path, desired_path)\n",
    "\n",
    "# print(\"Path to dataset files:\", desired_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "iris_data = pd.read_csv(f'{desired_path}/2/Iris.csv')\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://miro.medium.com/v2/format:webp/1*pO5X2c28F1ysJhwnmPsy3Q.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Simple Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Input layer (4 features) -> Hidden layer (36 neurons)\n",
    "        self.fc1 = nn.Linear(input_size, 36)\n",
    "        # Hidden layer (36 neurons) -> Hidden layer (24 neurons)\n",
    "        self.fc2 = nn.Linear(36, 24)\n",
    "        # Hidden layer (24 neurons) -> Output layer (3 classes)\n",
    "        self.fc3 = nn.Linear(24, 3)\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Softmax activation for multi-class classification\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the network\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation Function\n",
    "\n",
    "The **Rectified Linear Unit (ReLU)** is one of the most commonly used activation functions in deep learning, particularly for hidden layers in neural networks.\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the input to the neuron.\n",
    "- The output is $x$ if $x \\geq 0$, otherwise it is `0`.\n",
    "\n",
    "In piecewise form:\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } x < 0 \\\\\n",
    "x & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### What is ReLU?\n",
    "ReLU is a simple, piecewise linear function that outputs:\n",
    "- **0** when the input is negative.\n",
    "- The **input itself** when the input is positive.\n",
    "\n",
    "#### Why is ReLU Important in Hidden Layers?\n",
    "\n",
    "1. **Simplicity and Efficiency**:\n",
    "   - ReLU is computationally efficient because it involves simple operations like comparison and maximum, making it faster to compute than other activation functions like sigmoid or tanh.\n",
    "\n",
    "2. **Avoids Vanishing Gradient Problem**:\n",
    "   - Unlike sigmoid or tanh, ReLU does not saturate in the positive region, which helps mitigate the **vanishing gradient problem**. This allows gradients to propagate more effectively during backpropagation.\n",
    "\n",
    "3. **Sparsity**:\n",
    "   - ReLU introduces sparsity by setting negative values to zero, which means only a subset of neurons are active at any time. This can improve model efficiency and interpretability.\n",
    "\n",
    "4. **Better Performance**:\n",
    "   - Empirically, neural networks using ReLU tend to converge faster and often achieve better performance compared to those using sigmoid or tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "\n",
    "The **Softmax** function is commonly used in the output layer of neural networks for multi-class classification problems.\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "For an input vector $\\mathbf{z} = [z_1, z_2, \\dots, z_n]$, the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ is the $i$-th element of the input vector.\n",
    "- $n$ is the number of classes.\n",
    "- $e$ is Euler's number (approximately 2.718).\n",
    "\n",
    "#### What is Softmax?\n",
    "\n",
    "The **Softmax** function converts the raw output (logits) of a neural network into a probability distribution over multiple classes. The output values are in the range [0, 1], and the sum of all the probabilities is equal to 1.\n",
    "\n",
    "#### Why is Softmax Important in the Output Layer?\n",
    "\n",
    "1. **Probability Interpretation**:\n",
    "   - Softmax transforms the output logits into probabilities. Each output can be interpreted as the probability that the input belongs to a specific class, making it suitable for **multi-class classification**.\n",
    "\n",
    "2. **Ensures Outputs Sum to 1**:\n",
    "   - Softmax ensures that the sum of all output probabilities is exactly 1, making the outputs a valid probability distribution.\n",
    "\n",
    "3. **Facilitates Cross-Entropy Loss**:\n",
    "   - In classification tasks, **cross-entropy loss** is often used as the loss function. Softmax works seamlessly with cross-entropy by providing a differentiable probability output, which makes it easier to optimize the network during backpropagation.\n",
    "\n",
    "4. **Handles Multi-Class Classification**:\n",
    "   - Unlike activation functions like sigmoid, which are designed for binary classification, softmax is ideal when there are more than two classes, as it provides a probability distribution across all possible classes.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "If a neural network outputs logits `[2.0, 1.0, 0.1]`, the softmax function will transform these into probabilities like `[0.7, 0.2, 0.1]`, indicating the most likely class.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **Softmax** is essential for multi-class classification because it converts raw logits into a probability distribution.\n",
    "- It provides a clear, interpretable output where each class is assigned a probability.\n",
    "- Softmax works well with **cross-entropy loss**, making it a standard choice for the output layer in classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "X = iris_data.drop('Species', axis=1)  # Features (4 columns)\n",
    "y = iris_data['Species']  # Original string labels\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Use LabelEncoder to convert string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Convert string labels into numeric labels\n",
    "\n",
    "# Split the dataset into training and valing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Check the shapes of the splits to confirm they match\n",
    "# print(\"X_train shape:\", X_train.shape)  # Should be (120, 4)\n",
    "# print(\"y_train shape:\", y_train.shape)  # Should be (120,)\n",
    "# print(\"X_val shape:\", X_val.shape)    # Should be (30, 4)\n",
    "# print(\"y_val shape:\", y_val.shape)    # Should be (30,)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)  # Ensure y_train is in the correct format\n",
    "X_val_tensor = torch.Tensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "\n",
    "# # Verify tensor shapes\n",
    "# print(\"X_train_tensor shape:\", X_train_tensor.shape)  # Should be (120, 4)\n",
    "# print(\"y_train_tensor shape:\", y_train_tensor.shape)  # Should be (120,)\n",
    "# print(\"X_val_tensor shape:\", X_val_tensor.shape)    # Should be (30, 4)\n",
    "# print(\"y_val_tensor shape:\", y_val_tensor.shape)    # Should be (30,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) Formula:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$: Model parameters (weights).\n",
    "- $\\eta$: Learning rate (step size).\n",
    "- $\\nabla J(\\theta)$: Gradient of the loss function $J$ with respect to the parameters $theta$.\n",
    "\n",
    "### Explanation:\n",
    "1. **Gradient $\\nabla J(\\theta)$**: Computes the direction and magnitude to adjust the parameters.\n",
    "2. **Learning Rate $\\eta$**: Controls how much the parameters are updated.\n",
    "3. **Parameter Update**: $\\theta$ is updated in the direction that minimizes the loss.\n",
    "\n",
    "For SGD, the gradient is computed using a single random sample or a small batch instead of the entire dataset, making it computationally efficient.<br>\n",
    "\n",
    "![Alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/631731_P7z2BKhd0R-9uyn9ThDasA.webp)\n",
    "### Cross-Entropy Loss Formula:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $C$ is the number of classes.  \n",
    "- $y_i$ is the actual label (1 if class \\(i\\) is the correct class, 0 otherwise).  \n",
    "- $\\log(\\hat{y}_i)$ is the predicted probability for class \\(i\\).\n",
    "- This measures the difference between the predicted probabilities (after applying softmax) and the actual class labels. It's used to evaluate how well the predicted probabilities match the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleNN(input_size)\n",
    "criterion = nn.CrossEntropyLoss()  # Same loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # Changed to SGD\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Zero out gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights using SGD\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
    "\n",
    "# valing the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_val, predicted)\n",
    "    print(f'Accuracy on val set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluate the model on the val set\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_pred_tensor = model(X_val_tensor)\n",
    "    _, y_pred = torch.max(y_pred_tensor, 1)  # Get the predicted classes\n",
    "\n",
    "# Convert predictions and true labels to numpy arrays for scikit-learn functions\n",
    "y_pred = y_pred.numpy()\n",
    "y_val = y_val_tensor.numpy()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECDC-Neural-Networks-JustinL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
